# äºšé©¬é€Šé«˜æ¸©äº‹ä»¶è¯†åˆ«ä¸æ¸©åº¦é¢„æµ‹é¡¹ç›®

## ğŸŒ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨ä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œå¯¹äºšé©¬é€Šçƒ­å¸¦é›¨æ—çš„æœˆåº¦é«˜æ¸©äº‹ä»¶è¿›è¡Œè¯†åˆ«ï¼ˆåˆ†ç±»ä»»åŠ¡ï¼‰ä»¥åŠæ¸©åº¦è¿›è¡Œé¢„æµ‹ï¼ˆå›å½’ä»»åŠ¡ï¼‰ã€‚è¯¥åŒºåŸŸå—å››å¤§æ°”å€™æ¨¡æ€ï¼ˆENSOã€NAOã€TSAã€TNAï¼‰çš„å½±å“æ˜¾è‘—ï¼Œè¿‘å¹´æ¥æç«¯é«˜æ¸©äº‹ä»¶é¢‘å‘ï¼Œå¯¹ç”Ÿæ€ç³»ç»Ÿå’Œå…¨çƒæ°”å€™ç¨³å®šæ€§é€ æˆæ½œåœ¨å¨èƒã€‚

æœ¬é¡¹ç›®åŸºäº TensorFlow/Keras æ¡†æ¶ï¼Œå®Œæˆä»¥ä¸‹ä¸¤ä¸ªä¸»è¦ä»»åŠ¡ï¼š

- **ä»»åŠ¡ A â€“ åˆ†ç±»ä»»åŠ¡ï¼š** é¢„æµ‹æŸæœˆæ˜¯å¦å‘ç”Ÿé«˜æ¸©äº‹ä»¶ï¼ˆHotï¼‰ã€‚
- **ä»»åŠ¡ B â€“ å›å½’ä»»åŠ¡ï¼š** é¢„æµ‹è¯¥æœˆçš„å®é™…æ¸©åº¦å€¼ã€‚

---

## ğŸ“ æ•°æ®è¯´æ˜

é¡¹ç›®æä¾›äº†ä¸¤ä»½ä¸»è¦æ•°æ®é›†ï¼š

- `Amazon_temperature_student.csv`ï¼šåŒ…å«1982å¹´è‡³2022å¹´äºšé©¬é€Šåœ°åŒºçš„æœˆåº¦æ¸©åº¦æ•°æ®åŠ4ç§æ°”å€™æ¨¡æ€æŒ‡æ•°ï¼ˆENSOã€NAOã€TSAã€TNAï¼‰ã€‚
- `threshold.csv`ï¼šæä¾›æ¯ä¸ªæœˆä»½çš„é«˜æ¸©åˆ¤å®šé˜ˆå€¼ï¼ˆthresholdï¼‰ã€‚

æ°”å€™æ¨¡æ€æŒ‡æ•°å®šä¹‰å¦‚ä¸‹ï¼š

| æŒ‡æ•° | å«ä¹‰                                 | èŒƒå›´     |
|------|--------------------------------------|----------|
| ENSO | å„å°”å°¼è¯º/æ‹‰å°¼å¨œ (Nino 3.4åŒºæµ·æ¸©å¼‚å¸¸) | -3 åˆ° 3  |
| NAO  | äºšé€Ÿå°”ç¾¤å²›ä¸å†°å²›é—´çš„æ°”å‹å·®ï¼ˆå½’ä¸€åŒ–ï¼‰ | -4 åˆ° 4  |
| TSA  | çƒ­å¸¦å—å¤§è¥¿æ´‹æµ·æ¸©å¼‚å¸¸                  | -1 åˆ° 1  |
| TNA  | çƒ­å¸¦åŒ—å¤§è¥¿æ´‹æµ·æ¸©å¼‚å¸¸                  | -1 åˆ° 1  |

---

## ğŸ§  ä»»åŠ¡ A â€“ é«˜æ¸©äº‹ä»¶åˆ†ç±»

### ğŸ”§ æ•°æ®é¢„å¤„ç†

- æ„é€ äºŒå€¼æ ‡ç­¾ `Hot`ï¼šè‹¥è¯¥æœˆæ¸©åº¦è¶…è¿‡å¯¹åº”æœˆä»½é˜ˆå€¼ï¼Œåˆ™è®°ä¸º1ï¼Œå¦åˆ™ä¸º0ã€‚
- æ•°æ®é›†æŒ‰æ¯”ä¾‹éšæœºåˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
- ä½¿ç”¨ `StandardScaler` å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ã€‚
- å¯¹æœˆä»½ä½¿ç”¨**å‘¨æœŸæ€§ç¼–ç **ï¼Œä»¥åæ˜ æ—¶é—´è¿ç»­æ€§ï¼ˆå³ 12 æœˆä¸ 1 æœˆåº”ç›¸è¿‘ï¼‰ã€‚

### ğŸ—ï¸ æ¨¡å‹ç»“æ„

- å¤šå±‚å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œä½¿ç”¨ ReLU æ¿€æ´»å‡½æ•°ã€‚
- è¾“å‡ºå±‚ï¼š1 ä¸ªç¥ç»å…ƒï¼ŒSigmoid æ¿€æ´»å‡½æ•°ã€‚
- æŸå¤±å‡½æ•°ï¼šBinary Crossentropyï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰ã€‚
- ä¼˜åŒ–å™¨ï¼šAdamã€‚
- è¶…å‚æ•°é…ç½®ï¼š
  - æ‰¹å¤§å°ï¼š32
  - è®­ç»ƒè½®æ•°ï¼š50
  - å­¦ä¹ ç‡ï¼š0.001

### ğŸ“Š æ¨¡å‹è¯„ä¼°

- æµ‹è¯•é›†**å¹³è¡¡å‡†ç¡®ç‡ï¼ˆBalanced Accuracyï¼‰**ï¼š**87%**
- æ•æ„Ÿåº¦ï¼ˆSensitivity / True Positive Rateï¼‰ï¼š**84%**
- ç‰¹å¼‚åº¦ï¼ˆSpecificity / True Negative Rateï¼‰ï¼šç”±æ··æ·†çŸ©é˜µè®¡ç®—è·å¾—
- å¯è§†åŒ–æ··æ·†çŸ©é˜µ
- ç»˜åˆ¶è®­ç»ƒé›†ä¸éªŒè¯é›†çš„å‡†ç¡®ç‡å˜åŒ–æ›²çº¿

---

## ğŸŒ¡ï¸ ä»»åŠ¡ B â€“ æœˆåº¦æ¸©åº¦å›å½’é¢„æµ‹

### ğŸ”§ æ•°æ®é¢„å¤„ç†

- è¾“å…¥ç‰¹å¾ä½¿ç”¨ `StandardScaler` è¿›è¡Œæ ‡å‡†åŒ–ã€‚
- **éšæœºåˆ’åˆ†æ¨¡å¼ä¸‹**ï¼šç›®æ ‡å€¼ï¼ˆtemperatureï¼‰ä¸è¿›è¡Œç¼©æ”¾ã€‚
- **é€å¹´åˆ’åˆ†æ¨¡å¼ä¸‹**ï¼š
  - å°†å®Œæ•´å¹´ä»½åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼Œç¡®ä¿æ¯å¹´åªå‡ºç°åœ¨ä¸€ä¸ªå­é›†ä¸­ã€‚
  - å¯¹ç›®æ ‡å€¼ï¼ˆtemperatureï¼‰ä½¿ç”¨ `MinMaxScaler` è¿›è¡Œç¼©æ”¾ï¼Œä»…åœ¨è®­ç»ƒé›†æ‹Ÿåˆå¹¶ç”¨äºéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
  - ç‰¹å¾ç¼©æ”¾ç­–ç•¥ä¸éšæœºåˆ’åˆ†ä¿æŒä¸€è‡´ã€‚

### ğŸ—ï¸ æ¨¡å‹ç»“æ„

- å…¨è¿æ¥ç¥ç»ç½‘ç»œç”¨äºå›å½’é¢„æµ‹ã€‚
- è¾“å‡ºå±‚ï¼š1 ä¸ªç¥ç»å…ƒï¼Œçº¿æ€§æ¿€æ´»å‡½æ•°ã€‚
- æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰ã€‚
- ä¼˜åŒ–å™¨ï¼šAdamã€‚
- è¶…å‚æ•°é…ç½®ï¼š
  - æ‰¹å¤§å°ï¼š32
  - è®­ç»ƒè½®æ•°ï¼š100
  - å­¦ä¹ ç‡ï¼š0.001

### ğŸ“Š æ¨¡å‹è¯„ä¼°

- **éšæœºåˆ’åˆ†ç»“æœ**ï¼š
  - å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ï¼š**å°äº 0.27Â°C**
  - Pearson ç›¸å…³ç³»æ•°ï¼š**0.93**
  - ç»˜åˆ¶çœŸå®å€¼ä¸é¢„æµ‹å€¼çš„æ•£ç‚¹å›¾

- **é€å¹´åˆ’åˆ†ç»“æœ**ï¼š
  - ä½¿ç”¨ç›¸åŒç½‘ç»œç»“æ„ä¸è¶…å‚é‡æ–°è®­ç»ƒ
  - é¢„æµ‹ç»“æœç» MinMaxScaler åæ ‡å‡†åŒ–å¤„ç†
  - ä½¿ç”¨ MAE ä¸ Pearson r è¿›è¡Œè¯„ä¼°

---

## ğŸ” æ¨¡å‹éƒ¨ç½²ä¸å¤ç°æ€§ä¿éšœ

- å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯å¤ç°ã€‚
- ä¿å­˜æ‰€æœ‰æ¨¡å‹ä¸é¢„å¤„ç†å™¨ï¼š
  - Hot åˆ†ç±»å™¨ + ç‰¹å¾ç¼©æ”¾å™¨
  - éšæœºåˆ’åˆ†å›å½’æ¨¡å‹ + ç‰¹å¾ç¼©æ”¾å™¨
  - é€å¹´åˆ’åˆ†å›å½’æ¨¡å‹ + ç‰¹å¾ç¼©æ”¾å™¨ + ç›®æ ‡ç¼©æ”¾å™¨
- Notebook åŒ…å«ï¼š
  - ä¸€é”®åŠ è½½éšè—æµ‹è¯•é›†åŠæ‰€æœ‰æ¨¡å‹
  - è‡ªåŠ¨è¯„ä¼°å¹¶è¾“å‡ºæ··æ·†çŸ©é˜µã€å›å½’æ•£ç‚¹å›¾ã€å„é¡¹è¯„ä¼°æŒ‡æ ‡

---

## ğŸ“Œ é™„åŠ è¯´æ˜

- æœˆä»½ä½¿ç”¨ä»¥ä¸‹å…¬å¼è¿›è¡Œå‘¨æœŸæ€§ç¼–ç ï¼š  
  `month_norm = 2Ï€ Ã— (month - 1) / 12`  
  ä»¥ç¡®ä¿ 12 æœˆä¸ 1 æœˆåœ¨ç‰¹å¾ç©ºé—´ä¸­è·ç¦»æ¥è¿‘ã€‚

- æ¨¡å‹æ€»å‚æ•°é‡æ§åˆ¶åœ¨æ ·æœ¬æ•°çš„ 10% ä»¥ä¸‹ï¼Œé¿å…è¿‡æ‹Ÿåˆã€‚

---




# Amazon Hot Event Detection and Temperature Forecasting

## ğŸŒ Project Overview

This project focuses on predicting monthly temperature and identifying high-temperature events (hot events) in the Amazon rainforest using artificial neural networks. The region of studyâ€”highlighted in red on the map provided in the assignmentâ€”is particularly vulnerable to climate variability, especially due to the influence of four major oceanic climate modes: ENSO, NAO, TSA, and TNA.

The project addresses two main tasks using TensorFlow/Keras:

- **Task A â€“ Classification:** Predict whether a hot event occurs in a given month.
- **Task B â€“ Regression:** Predict the actual temperature for a given month.

---

## ğŸ“ Data Description

Two main datasets are provided:

- `Amazon_temperature_student.csv`: Contains monthly temperature records (1982â€“2022) and climate mode indices (ENSO, NAO, TSA, TNA).
- `threshold.csv`: Provides the hot event temperature threshold for each calendar month.

The climate mode indices have the following interpretations:

| Index | Meaning                                    | Range   |
|-------|---------------------------------------------|---------|
| ENSO  | El NiÃ±o/La NiÃ±a (Nino 3.4 SST anomaly)      | -3 to 3 |
| NAO   | Pressure differential (Azores - Iceland)    | -4 to 4 |
| TSA   | SST anomaly (Tropical South Atlantic)       | -1 to 1 |
| TNA   | SST anomaly (Tropical North Atlantic)       | -1 to 1 |

---

## ğŸ§  Task A â€“ Hot Event Classification

### ğŸ”§ Preprocessing

- A binary `Hot` label is defined: 1 if temperature exceeds the monthly threshold; 0 otherwise.
- Dataset is randomly split into training/validation/test sets.
- Features are normalized using `StandardScaler`.
- `month` is cyclically encoded to preserve temporal continuity.

### ğŸ—ï¸ Model Architecture

- Fully-connected neural network with ReLU activations.
- Output layer: 1 unit with sigmoid activation.
- Loss: Binary Crossentropy.
- Optimizer: Adam.
- Hyperparameters:
  - Batch size: 32
  - Epochs: 50
  - Learning rate: 0.001

### ğŸ“Š Evaluation

- Balanced Accuracy on test set: **87%**
- Sensitivity (TPR): **84%**
- Specificity (TNR): Calculated from confusion matrix
- Confusion matrix plotted for visualization
- Training and validation accuracy plotted across epochs

---

## ğŸŒ¡ï¸ Task B â€“ Temperature Regression

### ğŸ”§ Preprocessing

- Inputs normalized using `StandardScaler`.
- Target (`temperature`) not transformed in random split.
- For year-wise split:
  - Years partitioned such that each appears in only one of training/validation/test.
  - Target (`temperature`) scaled using `MinMaxScaler`, fitted only on the training set.

### ğŸ—ï¸ Model Architecture

- Fully-connected neural network for regression.
- Output: 1 unit (linear activation).
- Loss: Mean Squared Error.
- Optimizer: Adam.
- Hyperparameters:
  - Batch size: 32
  - Epochs: 100
  - Learning rate: 0.001

### ğŸ“Š Evaluation

- **Random Split:**
  - MAE: **< 0.27Â°C**
  - Pearson r: **0.93**
  - True vs Predicted scatter plot visualized

- **Year-wise Split:**
  - Same architecture re-used
  - Inverse transform applied to scaled predictions
  - Evaluation via MAE and Pearson r

---

## ğŸ” Deployment & Reproducibility

- Random seed fixed to ensure reproducibility
- All scalers and models saved:
  - Classifier + feature scaler
  - Random-split regressor + feature scaler
  - Year-wise regressor + feature scaler + target scaler
- Notebook includes:
  - Single cell to load hidden test dataset
  - Restore all saved models and scalers
  - Run evaluations & generate required plots and metrics

---

## ğŸ“Œ Notes

- `month` was cyclically encoded as:  
  `month_norm = 2Ï€ Ã— (month - 1) / 12`  
  This ensures that December and January are close in feature space.

- Model complexity was carefully kept under 10% of training sample size.

---
